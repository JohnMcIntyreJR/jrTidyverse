beer %>%
mutate(Ale = str_detect(URL, "Ale"),
Stout = str_detect(URL, "Stout"),
Ipa = str_detect(URL, "Ipa"),
yes = Ale + Stout + Ipa) %>%
filter(yes != 0) %>%
mutate(Ale = ifelse(Ale, "Ale", "no"),
Stout = ifelse(Stout, "Stout", "no"),
Ipa = ifelse(Ipa, "Ipa", "no")) %>%
select(-yes) %>%
gather(Type, yes, Ale, Stout, Ipa) %>%
filter(yes != "no") %>%
select(-yes) %>%
ggplot(aes(Color, ABV, colour = Type)) +
geom_point()
day = c("Wed", "Sat", "Fri", "Mon")
sort(day)
factor(day)
factor(day, levels = c("Mon", "Tue",
"Wed", "Thu",
"Fri", "Sat", "Sun"))
day = factor(day, levels = c("Mon", "Tue",
"Wed", "Thu",
"Fri", "Sat", "Sun"))
sort(day)
data(movies, package = "ggplot2movies")
movies %>%
group_by(mpaa) %>%
summarise(mean_r = mean(rating))
rating_mean = movies %>%
group_by(mpaa) %>%
summarise(mean_r = mean(rating))
ggplot(rating_mean) +
geom_point(aes(x = mpaa, y = mean_r))
library("forcats")
ggplot(rating_mean) +
geom_point(aes(x = fct_reorder(mpaa, mean_r),
y = mean_r))
ggplot(rating_mean) +
geom_point(aes(x = fct_reorder(mpaa, desc(mean_r)),
y = mean_r))
ggplot(rating_mean) +
geom_point(aes(x = fct_relevel(mpaa,
"", "PG",
"PG-13", "R",
"NC-17"),
y = mean_r))
ggplot(rating_mean) +
geom_point(aes(x = fct_recode(mpaa, "Unknown" = ""),
y = mean_r))
rating_mean %>%
mutate(mpaa = fct_recode(mpaa, "Unknown" = ""))
rating_mean %>%
mutate(mpaa = fct_recode(mpaa, "Unknown" = ""),
mpaa = fct_reorder(mpaa, desc(mean_r)))
rating_mean %>%
mutate(mpaa = fct_recode(mpaa, "Unknown" = ""),
mpaa = fct_reorder(mpaa, desc(mean_r))) %>%
ggplot() +
geom_point(aes(x = mpaa, y = mea_r))
rating_mean %>%
mutate(mpaa = fct_recode(mpaa, "Unknown" = ""),
mpaa = fct_reorder(mpaa, desc(mean_r))) %>%
ggplot() +
geom_point(aes(x = mpaa, y = mean_r))
rating_mean %>%
mutate(mpaa = fct_recode(mpaa, "Unknown" = ""),
mpaa = fct_collapse(mpaa,
Child = c("PG", "PG-13"),
Adult = c("R", "NC-17")))
rating_mean %>%
mutate(mpaa = fct_recode(mpaa, "Unknown" = ""),
mpaa = fct_collapse(mpaa,
Child = c("PG", "PG-13"),
Adult = c("R", "NC-17"))) %>%
group_by(mpaa) %>%
summarise(mean_r = mean(mean_r))
rating_mean %>%
mutate(mpaa = fct_recode(mpaa, "Unknown" = ""),
mpaa = fct_collapse(mpaa,
Child = c("PG", "PG-13"),
Adult = c("R", "NC-17"))) %>%
group_by(mpaa) %>%
summarise(mean_r = mean(mean_r)) %>%
ggplot() +
geom_point(aes(x = mpaa, y = mean_r))
library("tidytext")
data(dickens, package = "jrTidyverse2")
View(dickens)
?unnest_tokens
dickens %>%
unnest_tokens(word, text, token = "words")
dickens %>%
unnest_tokens(word, text, token = "words") %>%
View()
dickens = dickens %>%
unnest_tokens(word, text, token = "words")
dickens %>%
count(word)
dickens %>%
count(word) %>%
arrange(n)
dickens %>%
count(word) %>%
arrange(desc(n))
?get_stopwords
stopwords_getsources()
stopwords::stopwords_getsources()
get_stopwords()
get_stopwords() %>%
View()
get_stopwords(source = "smart") %>%
View()
sw = get_stopwords()
sw %>%
View(())
sw %>%
View()
anti_join(dickens, sw, by = "word")
dickens = anti_join(dickens, sw, by = "word")
dickens %>%
count(word) %>%
arrange(desc(n))
get_sentiments()
?get_sentiments
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "bing") %>%
View()
get_sentiments(lexicon = "nrc")
get_sentiments(lexicon = "nrc") %>%
View()
get_sentiments(lexicon = "loughran") %>%
View()
get_sentiments()
sent = get_sentiments()
inner_join(dickens, sent, by = "word")
dickens = inner_join(dickens, sent, by = "word")
dickens %>%
unnest_tokens(word, text, token = "words") %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments(), by = "word") %>%
group_by(book) %>%
summarise(mean(score))
data(dickens, package = "jrTidyverse2")
dickens %>%
unnest_tokens(word, text, token = "words") %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments(), by = "word") %>%
group_by(book) %>%
summarise(mean(score))
library("worldcloud")
library("wordcloud")
dickens = dickens %>%
unnest_tokens(word, text, token = "words") %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments(), by = "word")
dickens %>%
filter(book == "Oliver Twist")
dickens %>%
filter(book == "Oliver Twist") %>%
count(word)
dickens_count = dickens %>%
filter(book == "Oliver Twist") %>%
count(word)
wordcloud(dickens_count$word, dickens_count$n,
max.words = 100)
?wordcloud
wordcloud(dickens_count$word, dickens_count$n,
max.words = 100, random.order = FALSE)
wordcloud(dickens_count$word, dickens_count$n,
max.words = 100, random.order = FALSE)
wordcloud(dickens_count$word, dickens_count$n,
max.words = 100, random.order = FALSE,
ordered.colors = TRUE)
wordcloud(dickens_count$word, dickens_count$n,
max.words = 100, random.order = FALSE,
ordered.colors = TRUE,
random.color = TRUE)
wordcloud(dickens_count$word, dickens_count$n,
max.words = 100, random.order = FALSE,
colors = c("red", "blue"))
wordcloud(dickens_count$word, dickens_count$n,
max.words = 100, random.order = FALSE,
colors = c("red", "blue", "green"))
data(dickens, package = "jrTidyverse2")
dickens %>%
filter(book == "David Copperfield") %>%
unnest_tokens(bigram, text,
token = "ngrams",
n = 2)
dickens %>%
filter(book == "David Copperfield") %>%
unnest_tokens(bigram, text,
token = "ngrams",
n = 2) %>%
View()
dickens %>%
filter(book == "David Copperfield") %>%
unnest_tokens(bigram, text,
token = "ngrams",
n = 2) %>%
drop_na(bigram)
dickens = dickens %>%
filter(book == "David Copperfield") %>%
unnest_tokens(bigram, text,
token = "ngrams",
n = 2) %>%
drop_na(bigram)
dickens %>%
count(bigram)
dickens %>%
count(bigram, sort = TRUE)
dickens %>%
separate(bigram, c("word1", "word2"),
sep = " ")
dickens %>%
separate(bigram, c("word1", "word2"),
sep = " ")
dickens %>%
separate(bigram, c("word1", "word2"),
sep = " ") %>%
View()
sw = get_stopwords()
dickens =dickens %>%
separate(bigram, c("word1", "word2"),
sep = " ")
data(dickens, package = "jrTidyverse2")
dickens = dickens %>%
filter(book == "David Copperfield") %>%
unnest_tokens(bigram, text,
token = "ngrams",
n = 2) %>%
drop_na(bigram)
dickens =dickens %>%
separate(bigram, c("word1", "word2"),
sep = " ") %>%
filter(!word1 %in% sw$word) %>%
filter(!word2 %in% sw$word)
dickens %>%
count(word1, word2)
dickens %>%
count(word1, word2, sort = TRUE)
library("igraph")
dickens %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 15) %>%
graph_from_data_frame()
dickens_graph = dickens %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 15) %>%
graph_from_data_frame()
library("ggraph")
ggraph(dickens_graph, layout = "fr")
ggraph(dickens_graph, layout = "fr") +
geom_edge_link()
ggraph(dickens_graph, layout = "fr") +
geom_edge_link() +
geom_node_point()
ggraph(dickens_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name))
ggraph(dickens_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name),
vjust = 1,
hjust = 1)
vignette("forcats_solutions",
package = "jrTidyverse2")
vignette("purrr_solutions",
package = "jrTidyverse2")
list(c(1,2,3,4),1 , "a")
x = list(c(1,2,3,4),1 , "a")
x[1]
x[[1]]
dickens
dickens %>%
slice(1)
dickens %>%
slice(1:3)
dickens %>%
group_by(book) %>%
slice(1:3)
dickens %>%
group_by(chapter) %>%
slice(1:3)
library(jrTidyverse)
library(jrTidyverse)
data(okcupid, package = "jrTidyverse")
library("jrTidyverse")
okcupid = okcupid %>%
mutate(last_online = ymd_hms(last_online, tz = "America/Los_Angeles"))
library("tidyverse")
okcupid = okcupid %>%
mutate(last_online = ymd_hms(last_online, tz = "America/Los_Angeles"))
library(lubridate)
okcupid = okcupid %>%
mutate(last_online = ymd_hms(last_online, tz = "America/Los_Angeles"))
okcupid = okcupid %>%
mutate(week_day = wday(last_online, label = TRUE))
okcupid %>%
count(week_day)
library(jrTidyverse)
drat::addRepo("jr-packages")
install.packages("jrTidyverse")
vignette(package = "jrTidyverse")
library(jrTidyverse)
drat::addRepo("jr-packages")
install.packages("jrTidyverse")
install.packages("jrTidyverse")
vignette(package = "jrTidyverse")
drat::addRepo("jr-packages")
install.packages("jrTidyverse")
vignette(package = "jrTidyverse")
setwd("~/GitHub/Notes/jrTidyverse_notes")
library(jrTidyverse)
jrTidyverse:::get_got_path()
library(jrTidyverse)
library(jrTidyverse)
library(jrTidyverse)
install.packages("jrTidyverse")
ukGeom = readRDS(file = "../data/RDS/UK_shape.rds")
ukGeom =
ukGeom  %>%
mutate(pop_den_2015 = population_2015 / area,
pop_den_2017 = population_2017 / area,
pop_den_inc = (pop_den_2017 - pop_den_2015)/pop_den_2015)
library("sf")
library("tidyverse")
library("tidyverse")
library("tmap")#
ukGeom = readRDS(file = "../data/RDS/UK_shape.rds")
ukGeom =
ukGeom  %>%
mutate(pop_den_2015 = population_2015 / area,
pop_den_2017 = population_2017 / area,
pop_den_inc = (pop_den_2017 - pop_den_2015)/pop_den_2015)
tm_shape(uk) + tm_borders() + tm_fill("pop_den_inc")
tm_shape(ukGeom) + tm_borders() + tm_fill("pop_den_inc")
tm_shape(ukGeom) +
tm_borders() +
tm_fill("pop_den_inc")+
tm_layout(legend.outside = TRUE)
ukData = readRDS(file = "../data/RDS/UK_data.rds")
ukGeomFull = left_join(ukGeom, ukData, by = c("id" = "geo"))
library("tidyverse")
ukGeomFull = left_join(ukGeom, ukData, by = c("id" = "geo"))
ukGeom = readRDS(file = "../data/RDS/UK_shape.rds")
ukGeom =
ukGeom  %>%
mutate(pop_den_2015 = population_2015 / area,
pop_den_2017 = population_2017 / area,
pop_den_inc = (pop_den_2017 - pop_den_2015)/pop_den_2015)
tm_shape(ukGeom) +
tm_borders() +
tm_fill("pop_den_inc")+
tm_layout(legend.outside = TRUE)
library("sf")
library("sf")
library("tidyverse")
library("tmap")
ukGeom = readRDS(file = "../data/RDS/UK_shape.rds")
ukGeom =
ukGeom  %>%
mutate(pop_den_2015 = population_2015 / area,
pop_den_2017 = population_2017 / area,
pop_den_inc = (pop_den_2017 - pop_den_2015)/pop_den_2015)
tm_shape(ukGeom) +
tm_borders() +
tm_fill("pop_den_inc")+
tm_layout(legend.outside = TRUE)
ukData = readRDS(file = "../data/RDS/UK_data.rds")
ukGeomFull = left_join(ukGeom, ukData, by = c("id" = "geo"))
ukGeomFull
class(ukGeomFull)
class(ukGeomFull)
right_join(ukGeom, ukData, by =c("id" = "geo")) %>%
class()
right_join(ukData, ukGeom, by =c("id" = "geo")) %>%
class()
right_join(ukData, ukGeom, by =c("geo" = "id")) %>%
class()
tmp = right_join(ukData, ukGeom, by =c("geo" = "id"))
head(tmp)
tmp$geometry
tmap(tmp)
tm_shape(tmp)
tm_shape(tmp) + tm_borders()
ukGeomFull2 = inner_join(ukData, ukGeom, by = c("geo" = "id")
ukGeomFull2 = inner_join(ukData, ukGeom, by = c("geo" = "id"))
class(ukGeomFull2)
ukGeomFull2 = inner_join(ukData, ukGeom, by = c("geo" = "id"))
class(ukGeomFull2)
class(ukGeomFull)
tmap(ukGeomFull) + tm_borders()
tm_shape(ukGeomFull) + tm_borders()
tm_shape(ukGeomFull2) + tm_borders()
ukGeomFull %>%
st_drop_geometry() %>%
summarise(maxlifeExp = max(lifeExp),
minlifeExp = min(lifeExp),
maxUnemploy = max(unemployment),
maxUnemploy = min(unemployment))
ukGeomFull %>%
summarise(maxlifeExp = max(lifeExp),
minlifeExp = min(lifeExp),
maxUnemploy = max(unemployment),
maxUnemploy = min(unemployment))
ukGeomFull %>%
st_drop_geometry() %>%
summarise(maxlifeExp = max(lifeExp),
minlifeExp = min(lifeExp),
maxUnemploy = max(unemployment),
maxUnemploy = min(unemployment))
uk %>%
filter(unemployment == max(unemployment))
ukGeom %>%
filter(unemployment == max(unemployment))
ukGeomFull %>%
filter(unemployment == max(unemployment))
ukGeomFull %>%
st_drop_geometry() %>%
filter(unemployment == max(unemployment))
uk %>%
filter(population_2017 > 2500000) %>%
tm_shape() +
tm_borders() +
tm_text("region")
ukGeomFull %>%
filter(population_2017 > 2500000) %>%
tm_shape() +
tm_borders() +
tm_text("region")
ukGeomFill %>%
filter(income < 15000 | unemployment > 5) %>%
tm_shape() +
tm_fill(col = "gray") +
tm_text("region") +
tm_shape(uk) +
tm_borders()
ukGeomFull %>%
filter(income < 15000 | unemployment > 5) %>%
tm_shape() +
tm_fill(col = "gray") +
tm_text("region") +
tm_shape(uk) +
tm_borders()
ukGeomFull %>%
filter(income < 15000 | unemployment > 5) %>%
tm_shape() +
tm_fill(col = "gray") +
tm_text("region") +
tm_shape(uGeomFull) +
tm_borders()
ukGeomFull %>%
filter(income < 15000 | unemployment > 5) %>%
tm_shape() +
tm_fill(col = "gray") +
tm_text("region") +
tm_shape(ukGeomFull) +
tm_borders()
uk %>%
st_drop_geometry() %>%
group_by(country) %>%
summarise(total_pop = sum(population_2017),
avg_life = round(mean(lifeExp)))
ukGeomFull %>%
st_drop_geometry() %>%
group_by(country) %>%
summarise(total_pop = sum(population_2017),
avg_life = round(mean(lifeExp)))
ukGeomFull %>%
st_drop_geometry() %>%
group_by(country) %>%
summarise(total_pop = mean(income),
avg_life = round(mean(lifeExp)))
ukGeomFull %>%
st_drop_geometry() %>%
group_by(country) %>%
summarise(avg_income= round(mean(income)),
avg_life = round(mean(lifeExp)))
names(ukGeomFull)
ukGeomFull %>%
st_drop_geometry() %>%
group_by(country) %>%
summarise(avg_income= round(mean(births)),
avg_life = round(mean(lifeExp)))
ukGeomFull %>%
st_drop_geometry() %>%
group_by(country) %>%
summarise(avg_income= round(mean(unemployment)),
avg_life = round(mean(lifeExp)))
ukGeomFull %>%
st_drop_geometry() %>%
group_by(country) %>%
summarise(total_pop = sum(population_2017),
avg_life = round(mean(lifeExp)))
uk = readRDS(file = "../data/RDS/UK_shape.rds")
hills = readRDS("../data/RDS/UK_hills.rds")
hill
hills
ncol(hills)
names(hills)
hills$Ma
hills = readRDS("../data/RDS/UK_hills.rds")
hills =
hills %>%
st_as_sf(coords = c("Longitude", "Latitude"))
st_crs(hills) = st_crs(uk)
names(hills)
